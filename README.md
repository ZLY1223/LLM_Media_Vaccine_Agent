# ğŸ§  LLM_Media_Vaccine_Agent

This repository contains experimental notebooks and scripts for simulating COVID-19 vaccine decision-making using **Large Language Models (LLMs)** â€” including **GPT-4o-mini**, **GPT-4.1**, **Gemini-2.5-Flash**, and **LLaMA-4-17B**.  
Each notebook corresponds to a distinct model configuration (M0â€“M2) or analysis stage in the study.

---

## ğŸ“š Project Overview

This project investigates how **demographics**, **prior beliefs**, and **media diet** jointly influence simulated vaccine decisions.  
By comparing multiple LLM families (OpenAI, Google, and open-source), we examine how model reasoning and information inputs shape behavioral predictions.

| Model | Description | Key Input Features |
|--------|--------------|--------------------|
| **M0 â€“ Demographics Only** | Baseline model that predicts vaccine decisions purely from demographic attributes. | Age, gender, income, education, region |
| **M1 â€“ Survey Model** | Adds prior-belief variables on top of demographics (survey-based). | Demographics + attitudes toward vaccine, trust in science, perceived severity, etc. |
| **M2 â€“ Media Diet Model** | Incorporates personalized media diet on top of demographics (replacing belief inputs). | Demographics + Media Diet (Left Echochamber / Center-ish / Right Echochamber / Misinformation-Only) |

---

## ğŸ§© Repository Structure
```
LLM_Media_Vaccine_Agent/
â”œâ”€â”€ data/ # Example input data and results
â”œâ”€â”€ Chatgpt.ipynb # Main pipeline for GPT-4o-mini / GPT-4.1 / Gemini-2.5-Flash
â”œâ”€â”€ Llama_4_17b.ipynb # LLaMA-4-17B-based experiments
â”œâ”€â”€ Counterfactual_Analysis.ipynb # M2 counterfactual variants (random diet, random articles, etc.)
â”œâ”€â”€ TextGrad.ipynb # Prompt optimization via TextGrad (trainable segments between % Task Prompt and Output format)
â””â”€â”€ README.md
```

---

## ğŸ” Notebook Descriptions

### ğŸ¤– **Chatgpt.ipynb**
Implements all **M0â€“M2** configurations using **GPT-4o-mini** and **GPT-4.1**.  
- **M0 â€“ Demographics-Only:** baseline simulation using demographic attributes  
- **M1 â€“ Survey Model:** adds prior belief variables to demographics  
- **M2 â€“ Media Diet Model:** integrates demographics, beliefs, and media-exposure context  
  - Media diet assignment derived from participantsâ€™ demographics and beliefs  
  - Produces baseline results for comparison with Gemini-2.5-Flash and LLaMA-4-17B  

---

### ğŸŒ **Gemini_2_5_flash.ipynb**
Runs the same M0â€“M2 pipeline on **Gemini-2.5-Flash** (Google model).  
- Mirrors the ChatGPT notebook to ensure architecture-consistent prompts and evaluation  
- Provides a **cross-vendor comparison** between OpenAI (GPT) and Google (Gemini) systems  
- Evaluates robustness, reasoning diversity, and consistency across media-exposure conditions  

---

### ğŸ¦™ **Llama_4_17b.ipynb**
Runs the same experiments on **LLaMA-4-17B** (open-weight model via Hugging Face).  
Serves as the **open-source baseline** to benchmark against closed-source families (GPT / Gemini).  

---

### âš™ï¸ **TextGrad.ipynb**
Implements **prompt-engineering optimization** using **TextGrad**, applied to all LLM families (GPT, Gemini, LLaMA).  
- Optimizes reasoning within the `% Task Prompt` section  
- Adjusts trainable segments between `% Task Prompt` and `Output format`  
- Evaluated by accuracy, balanced accuracy, and recall across M2 settings  

---

### ğŸ” **Counterfactual_Analysis.ipynb**
Implements multiple **M2 counterfactual variants** across all models (GPT-4o-mini / GPT-4.1 / Gemini-2.5-Flash / LLaMA-4-17B):  
- **Random Diet Assignment** â€“ random media exposure per participant  
- **Random 5 Articles Sampling** â€“ five random articles per diet  
- **Random 10 Articles Sampling** â€“ ten random articles per diet  
- **Demographics-Only Assignment** â€“ removes belief layer  
- **Misinformation-Only** â€“ isolates misinformation exposure  
- **Public-Health-Only** â€“ isolates official health-source exposure  

These experiments assess how media-exposure variability and model family jointly shape simulated vaccine decisions.

---

## ğŸ’¾ Data

All data used in this repository are organized under the `data/` directory.
```
data/
â”œâ”€â”€ Input/
â”‚ â”œâ”€â”€ sample_1000/ # Example inputs for model inference
â”‚ â”œâ”€â”€ Media Diet Data.zip # Curated media diet data (left/center/right/misinformation/public health)
â”‚ â”œâ”€â”€ Rawdata_text.xlsx # Raw text and metadata used to construct prompts
â”‚ â””â”€â”€ README.md # Data documentation and preprocessing notes
â”‚
â””â”€â”€ Results/
â”œâ”€â”€ sample_1000/ # Example model outputs (GPT / Gemini / LLaMA comparisons)
â”œâ”€â”€ Media Diet.zip # Aggregated M2 results by media condition
â””â”€â”€ RAG.zip # Retrieval-augmented generation outputs and logs
```

- **Input/** â€” contains preprocessed demographic, belief, and media-exposure data used as model inputs.  
  - `Rawdata_text.xlsx` stores the text corpus and metadata for constructing media diets.  
  - `Media Diet Data.zip` includes categorized article samples for the five diet types.  
  - `sample_1000/` provides a minimal working subset for quick testing.

- **Results/** â€” stores representative outputs generated by each LLM.  
  - Includes sample runs for M0â€“M2, counterfactual experiments, and prompt-optimized (TextGrad) results.  
  - Each `.zip` archive corresponds to a specific experiment or model configuration.

> âš ï¸ Full-scale data (original survey-level inputs and all model generations) are not hosted here due to privacy and size constraints.  
> Researchers interested in reproducing full experiments may contact the author to request access.

---

## ğŸ§  Models Used
- **GPT-4o-mini / GPT-4.1** â€” OpenAI API  
- **Gemini-2.5-Flash** â€” Google API  
- **LLaMA-4-17B** â€” via Hugging Face Router  
- **TextGrad** â€” for prompt optimization and reasoning-segment fine-tuning  

---

## âš™ï¸ Environment Setup
Install dependencies:
```bash
pip install -r requirements.txt
